<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>AI Archaeology 3D Viewer</title>
<style>
  body {
    margin: 0;
    background: linear-gradient(180deg, #101010, #161616);
    color: #fff;
    font-family: 'Segoe UI', Roboto, sans-serif;
    overflow: hidden;
  }

  #frame {
    position: absolute;
    top: 40px;
    left: 40px;
    right: 300px;
    bottom: 40px;
    border: 4px solid #00bfff;
    border-radius: 10px;
    box-shadow: 0 0 25px rgba(0, 191, 255, 0.4);
    overflow: hidden;
  }

  canvas { width: 100%; height: 100%; display: block; }

  #toolbar {
    position: absolute; top: 10px; left: 50%; transform: translateX(-50%);
    display: flex; gap: 10px; padding: 8px 14px;
    background: rgba(0,0,0,0.7); border-radius: 8px;
    box-shadow: 0 0 15px rgba(0,0,0,0.4);
    backdrop-filter: blur(6px);
  }

  .btn {
    border: none; border-radius: 5px;
    background: linear-gradient(90deg, #007bff, #00bfff);
    color: #fff; font-weight: 600;
    padding: 8px 14px; cursor: pointer;
  }

  .btn:hover { transform: scale(1.05); }

  #recommend-panel {
    position: absolute;
    top: 40px; right: 40px; bottom: 40px;
    width: 240px;
    background: rgba(0,0,0,0.75);
    border-radius: 10px;
    box-shadow: 0 0 15px rgba(0, 191, 255, 0.5);
    padding: 15px;
    overflow-y: auto;
  }

  #recommend-panel h2 {
    text-align: center;
    color: #00bfff;
    margin-top: 0;
  }

  #recommendations {
    font-size: 14px;
    line-height: 1.5;
  }

  input[type="file"] {
    color: white;
  }
</style>
</head>

<body>
<div id="toolbar">
  <input type="file" id="file-input" accept=".glb,.gltf,image/*" multiple>
  <button class="btn" id="reset-btn">Reset</button>
  <button class="btn" id="clear-btn">Clear</button>
  <button class="btn" id="rotate-btn">Rotate 90Â°</button>
</div>

<div id="frame"><canvas id="webgl"></canvas></div>

<div id="recommend-panel">
  <h2>AI Archaeology</h2>
  <p>Upload images or 3D models of artifacts to see automatic recommendations.</p>
  <hr />
  <div id="recommendations">Awaiting uploads...</div>
</div>

<!-- 3D + AI imports -->
<script type="importmap">
{
  "imports": {
    "three": "https://unpkg.com/three@0.163.0/build/three.module.js",
    "three/addons/": "https://unpkg.com/three@0.163.0/examples/jsm/"
  }
}
</script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.11.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet"></script>

<script type="module">
import * as THREE from 'three';
import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
import { DragControls } from 'three/addons/controls/DragControls.js';

let scene, camera, renderer, orbit, drag;
const models = [];
let mobileNetModel;

// Load TensorFlow model
(async function initAI() {
  mobileNetModel = await mobilenet.load();
  document.getElementById('recommendations').innerHTML = "AI ready. Please upload an image or model.";
})();

initScene();
animate();

function initScene() {
  const canvas = document.getElementById('webgl');
  scene = new THREE.Scene();
  camera = new THREE.PerspectiveCamera(70, canvas.clientWidth / canvas.clientHeight, 0.1, 1000);
  camera.position.set(4, 3, 7);

  renderer = new THREE.WebGLRenderer({ canvas, antialias: true });
  renderer.setSize(canvas.clientWidth, canvas.clientHeight);
  renderer.outputColorSpace = THREE.SRGBColorSpace;

  scene.add(new THREE.HemisphereLight(0xffffff, 0x444444, 1.8));
  scene.add(new THREE.AmbientLight(0xffffff, 1.2));

  orbit = new OrbitControls(camera, renderer.domElement);
  orbit.enableDamping = true;

  const grid = new THREE.GridHelper(40, 40, 0x555555, 0x333333);
  scene.add(grid);

  window.addEventListener('resize', () => {
    camera.aspect = canvas.clientWidth / canvas.clientHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(canvas.clientWidth, canvas.clientHeight);
  });

  document.getElementById('file-input').addEventListener('change', handleUpload);
  document.getElementById('reset-btn').addEventListener('click', resetCamera);
  document.getElementById('clear-btn').addEventListener('click', clearScene);
  document.getElementById('rotate-btn').addEventListener('click', () => rotateCamera(90));
}

function animate() {
  requestAnimationFrame(animate);
  orbit.update();
  renderer.render(scene, camera);
}

function resetCamera() {
  orbit.reset();
  fitCameraToObjects();
}

function clearScene() {
  models.forEach(m => scene.remove(m));
  models.length = 0;
  document.getElementById('recommendations').innerHTML = "Scene cleared.";
}

function rotateCamera(deg) {
  camera.position.applyAxisAngle(new THREE.Vector3(0,1,0), THREE.MathUtils.degToRad(deg));
  orbit.update();
}

async function handleUpload(event) {
  const files = event.target.files;
  for (const file of files) {
    if (file.name.match(/\.(glb|gltf)$/i)) {
      loadGLTF(file);
    } else if (file.type.startsWith('image/')) {
      const imgURL = URL.createObjectURL(file);
      const recs = await analyzeArtifact(imgURL);
      URL.revokeObjectURL(imgURL);
      showRecommendations(file.name, recs);
    }
  }
}

function loadGLTF(file) {
  const loader = new GLTFLoader();
  const url = URL.createObjectURL(file);

  loader.load(url, (gltf) => {
    const model = gltf.scene;
    model.traverse((child) => {
      if (child.isMesh && child.material) {
        child.material.envMap = null;
        child.material.metalness = 0;
        child.material.roughness = 1;
        child.material.needsUpdate = true;
      }
    });
    scene.add(model);
    models.push(model);
    URL.revokeObjectURL(url);

    updateDragControls();
    fitCameraToObjects();
    document.getElementById('recommendations').innerHTML =
      "3D model loaded. You can rotate and move it. Try uploading artifact images for analysis.";
  });
}

function updateDragControls() {
  if (drag) drag.deactivate();
  drag = new DragControls(models, camera, renderer.domElement);
  drag.addEventListener('dragstart', () => orbit.enabled = false);
  drag.addEventListener('dragend', () => orbit.enabled = true);
}

function fitCameraToObjects() {
  if (!models.length) return;
  const box = new THREE.Box3();
  models.forEach(o => box.expandByObject(o));
  const center = box.getCenter(new THREE.Vector3());
  const size = box.getSize(new THREE.Vector3()).length();
  orbit.target.copy(center);
  camera.position.copy(center.clone().add(new THREE.Vector3(size, size, size)));
  orbit.update();
}

// --- AI PART ---
async function analyzeArtifact(imageURL) {
  if (!mobileNetModel) return ["AI model not ready."];
  const img = document.createElement('img');
  img.src = imageURL;
  await new Promise(res => { img.onload = res; });

  const predictions = await mobileNetModel.classify(img);
  const result = predictions.map(p => `${p.className} (${(p.probability * 100).toFixed(1)}%)`);

  // Map results to archaeological recommendations
  const hint = [];
  if (predictions.some(p => /pot|jar|ceramic|vase/.test(p.className)))
    hint.push("This might belong to Pottery or Ceramic Culture â€” check Mesopotamian, Greek, or Roman origins.");
  if (predictions.some(p => /metal|sword|helmet|tool|shield/.test(p.className)))
    hint.push("Potential metallic artifact â€” Compare metallurgy with Bronze or Iron Age artifacts.");
  if (predictions.some(p => /statue|stone|relic|bust/.test(p.className)))
    hint.push("Consider Hellenistic or Egyptian sculpture typologies for reference.");
  if (!hint.length) hint.push("General artifact: use shape and material density for period classification.");

  return result.concat("<br><br>AI Archaeologist Insight:<br>" + hint.join("<br>"));
}

function showRecommendations(name, recs) {
  document.getElementById('recommendations').innerHTML =
    `<b>ðŸ“œ File:</b> ${name}<br><br>${recs.join("<br>")}`;
}
</script>
</body>
</html>
